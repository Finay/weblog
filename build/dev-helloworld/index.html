<!doctype html>

<!-- Head -->
<head>
    <link rel="stylesheet" href="/weblog/static/style.css">
<link rel="icon" type="image/x-icon" href="/weblog/static/images/favicon.ico">
    <title> Hello, World! - kian's weblog</title>
</head>

<body>
<section class="content">
    <!-- Navigation -->
    <nav id="main-navigation">
    <a id="blog-title" href="/weblog/">
        <div>
            <h1>kian's weblog</h1>
        </div>
    </a>
    <div id="navbar">
        <ul>
            <li><a href="/weblog/">Home</a></li>
            <li><a href="/weblog/tags/">Tags</a></li>
            <li><a href="/weblog/logs/">Logs</a></li>
            <li><a href="/weblog/about/">About</a></li>
        </ul>
    </div>
</nav>

    <section class="post">
        
<h1>Hello, World!</h1>
<p>
    Capital H and W, a comma after the hello and an exclamation mark after the world. I'm convinced that is the only
    correct way to write the iconic starting message. Don't ask me why, I just think it has to be that way. I know it's
    not like those little traits matter when all you are looking for is a response from the computer but something about
    the exactness of repeating it that way across all first executions I run is satisfying.
    <br/><br/>
    Anyway, this post isn't meant to primarily be about how I like my "Hello, World!"s. You may have noticed the
    <i>dev</i> tag on this post. Yes, this is the first of hopefully many posts on computer science related posts, from
    just fun stuff to the technical. Today I did some AI experiments, for the first time since the fresh start of day
    94, towards making a model to generate music. I don't mean for this post to be technical (though I do want someone
    to talk about the technical bits with so please if you are curious or can help me reach out!!) so I'll just give a
    brief overview of my progress so far. I trained a basically out of the box transformer on a dataset of note
    sequences. A transformer is the same architecture that ChatGPT uses (the T of GPT means transformer) and is based on
    a series of layers consisting of attention, a mechanism that shares contextual information between individual points
    of the input data, and a feed forward network which does classical neural network stuff. I'm still trying to figure
    out what inputs and outputs a final model should take as it's quite tricky. I'm thinking the model will have to
    generate the song sequentially. In which case, I need to figure out if I'm meant to ask the model at each possible
    moment a note could be. Also even with the simplifying assumption that the model just needs to predict what to play
    given a moment, the potential vocabulary of outputs is enormous if you consider the combinations of various pitches,
    durations and whatever other data is necessary for the song. I started by avoiding the problem and assuming all
    songs are flat, as in they play one note at a time (from A to G#, only one octave), at every beat, for exactly one
    beat. It isn't a realistic assumption, but it simplifies my initial model to simply take a sequence of past notes
    and predict the next, perfect for a transformer. I made exactly that, exported the music and uhhh, it sounded crap.
    No surprise there but I am pretty sure it is better than random notes. Realizing I said this would be brief and it
    clearly isn't, so I'll talk more about my ideas and next steps in another post.
    <br/><br/>
    <img src="/weblog/static/images/ramiel_melody1.webp"/>
    Melody generated by the initial version of the model.
</p>

    </section>

    <!-- Footer -->
    
</section>
</body>